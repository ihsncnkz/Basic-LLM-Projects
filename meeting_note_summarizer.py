# -*- coding: utf-8 -*-
"""Meeting Note Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JJbc4d40TbSODKqZ4PxmR0RGxvjVLANY

# Introduction
In this chapter, I will develop an AI assistant for summarizing meeting notes. Some companies record audio during meetings. And I will transcribe these audio recordings into text. Then, My AI assistant will create summarisation from that text.
"""

# I download the python librarys for this project.
!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2

import os
import requests
from IPython.display import Markdown, display, update_display
from openai import OpenAI
from google.colab import drive, userdata
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig
import torch

AUDIO_MODEL = "whisper-1"
LLAMA = "meta-llama/Meta-Llama-3.1-8B-Instruct"

drive.mount("/content/drive")
audio_filename = "/content/drive/MyDrive/LLMS/Data/Audio/denver_extract.mp3"

hf_token = userdata.get('HF_TOKEN')
login(hf_token, add_to_git_credential = True)

openai_api_key = userdata.get('OPENAI_API_KEY')
openai = OpenAI(api_key = openai_api_key)

# I transcribe this audio recording into text.
audio_file = open(audio_filename, "rb")
transcription = openai.audio.transcriptions.create(model = AUDIO_MODEL, file = audio_file, response_format = 'text')
print(transcription)

# Prompts for AI assistant
system_message = "You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown."
user_prompt = f"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\n{transcription}"

messages = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": user_prompt}
  ]

# We are reducing the size of the AI ​​model to take up less space in memory.
quant_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_use_double_quant = True,
    bnb_4bit_compute_dtype = torch.float16,
    bnb_4bit_quant_type = 'nf4',
)

# Load the tokenizer from the pre-trained LLAMA model
tokenizer = AutoTokenizer.from_pretrained(LLAMA)

# Set the padding token to be the same as the end-of-sequence token
tokenizer.pad_token = tokenizer.eos_token

# Convert the chat messages to tokenized input tensors and move to GPU
inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')

# Initialize a streamer to display output text as it is generated
streamer = TextStreamer(tokenizer)

# Load the LLAMA model with automatic device mapping and quantization
model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map='auto', quantization_config=quant_config)

# Generate a response with a maximum of 2000 new tokens and stream the output
outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)

response = tokenizer.decode(outputs[0])
display(Markdown(response))

"""# CONCLUSION
We have come to the end of yet another LLM project. I used two LLM models. These are whisper-1 from the Openai and Meta-Llama-3.1-8B-Instruct from the Meta. I downloaded this Meta-Llama-3.1-8B-Instruct model at the HuggingFace. If you use that model you can use the HuggingFace. Thank you for taking a look at my project. I will continue to share projects. If you want to be informed in advance, you can follow me from the links below.

[LinkedIn](https://www.linkedin.com/in/ihsancenkiz/)<br>
[GitHub](https://github.com/ihsncnkz)<br>
[Kaggle](https://www.kaggle.com/ihsncnkz)
"""